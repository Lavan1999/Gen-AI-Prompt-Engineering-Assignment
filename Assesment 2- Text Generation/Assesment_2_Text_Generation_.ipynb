{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement 2: Text Generation\n",
        "Problem: Create a basic text generation model using a pre-trained transformer (e.g., GPT-3). Requirements:\n",
        "* Use the Hugging Face Transformers library.\n",
        "* Generate coherent text based on a given prompt. Evaluation Criteria:\n",
        "* Ability to load and use pre-trained models.\n",
        "* Quality and coherence of the generated text.\n",
        "* Understanding and application of the transformer model.\n"
      ],
      "metadata": {
        "id": "UsGeqryeuuT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Hugging Face Transformers and Tokenizers"
      ],
      "metadata": {
        "id": "xM2OdZBxtHL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kRl2c2JuYd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420904b8-31c0-424f-93f3-5d07713a77af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model"
      ],
      "metadata": {
        "id": "_SnpLtjaDSIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Set your Hugging Face token\n",
        "hf_token = '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'\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"gpt2\"  # Use a smaller model for demonstration if necessary\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hf_token)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps_qWpNU0MBF",
        "outputId": "488011ec-1314-4969-c30e-d108c597f571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Text"
      ],
      "metadata": {
        "id": "uMeP9Fu-DPQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Text\n",
        "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
        "    # Set the pad token to be the same as the eos token if not already set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Tokenize the input prompt with padding and truncation\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Move tensors to the correct device\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Generate text using the model\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        do_sample=True,\n",
        "        no_repeat_ngram_size=2,  # To Avoid repeating n-grams\n",
        "        temperature=0.7,  # Controls the randomness of predictions\n",
        "        top_k=50,  # Limits the number of tokens to sample from\n",
        "        top_p=0.95  # Limits the cumulative probability for sampling tokens\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Once upon a time in a land far, far away\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(generated_text[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vngmu_G72FA",
        "outputId": "f7420920-73d6-4ec6-c16d-a118f6744c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a land far, far away, the wind could blow all around you with just one strike. The first thing you would notice in this situation is that it seems to be the first time you've ever felt so powerless in your life. You feel so completely powerless to the world.\n",
            "\n",
            "And you don't know what that feels like. It feels so much like being in the moment. I'm sitting in my chair, and I can't feel my body moving. My\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prompt\n",
        "prompt = \"In a world where technology has evolved to control emotions, a scientist discovers a way to unlock hidden feelings. What happens next?\"\n",
        "\n",
        "# Generate text based on the new prompt\n",
        "generated_text = generate_text(prompt)\n",
        "print(generated_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN-06X0b8xrG",
        "outputId": "d92a7284-3cea-4677-ef55-22e955daff61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a world where technology has evolved to control emotions, a scientist discovers a way to unlock hidden feelings. What happens next?\n",
            "\n",
            "The film, which is currently out in theaters, is set for release in 2016.\n",
            " (Vincent D'Onofrio, Screen Rant)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate and print text for various prompts\n",
        "def test_prompts(prompts, max_length=100, num_return_sequences=1):\n",
        "    for prompt in prompts:\n",
        "        generated_text = generate_text(prompt, max_length=max_length, num_return_sequences=num_return_sequences)\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Generated Text: {generated_text[0]}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "# Example prompts\n",
        "prompts = [\n",
        "    \"Describe a futuristic city where humans and robots coexist harmoniously.\",\n",
        "    \"Imagine a conversation between Julius Caesar and Cleopatra discussing their strategies.\",\n",
        "    \"Explain the concept of quantum entanglement in simple terms for a high school student.\"\n",
        "]\n",
        "\n",
        "test_prompts(prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l78DAHF8AIcx",
        "outputId": "6ffc5048-69c4-4671-9b01-ae48e2906a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Describe a futuristic city where humans and robots coexist harmoniously.\n",
            "Generated Text: Describe a futuristic city where humans and robots coexist harmoniously.\n",
            "\n",
            "A futuristic City of Death: The final film in the series, the film chronicles the history of the human race. Directed by Michael Arndt (Alien), the movie is based on the book, The Last of Us. The film's director, Peter Berg (The Martian), will direct.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Imagine a conversation between Julius Caesar and Cleopatra discussing their strategies.\n",
            "Generated Text: Imagine a conversation between Julius Caesar and Cleopatra discussing their strategies. He has a long list of things that they want to do and are planning to accomplish and they are doing them to their advantage.\n",
            "\n",
            "And so, as they talk, Julius and his friends, in their way, have a much better idea of what they're talking about. They say, \"We want a plan.\" And they come up with it. And then they all get along and try to come to grips with some\n",
            "==================================================\n",
            "Prompt: Explain the concept of quantum entanglement in simple terms for a high school student.\n",
            "Generated Text: Explain the concept of quantum entanglement in simple terms for a high school student.\n",
            "\n",
            "A very simple model that uses quantum teleportation as an example of how quantum information can be transmitted between two objects is called a quantum state machine. This model was developed by scientists at the University of Chicago in Chicago, and it was presented at a meeting of the Society for Quantum Information in 2011. It is still in its early stages, but in the next few years we will be able to demonstrate that quantum\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Model\n",
        "# 1. BLEU Score\n",
        "\n",
        "BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating the quality of text generated by comparing it to one or more reference texts. It’s commonly used for machine translation and text generation tasks.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dhqHYMpQAXNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jAzZqgvAWx0",
        "outputId": "576283c1-5dde-4931-af7f-09d5af7aec53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Example reference and candidate texts\n",
        "references = [\n",
        "    [['this', 'is', 'a', 'test'], ['this', 'is', 'test']],\n",
        "    [['another', 'test']]\n",
        "]\n",
        "candidates = [\n",
        "    ['this', 'is', 'a', 'test'],\n",
        "    ['another', 'test']\n",
        "]\n",
        "\n",
        "# Compute BLEU score\n",
        "bleu_score = corpus_bleu(references, candidates)\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9xt-jrDAWv_",
        "outputId": "496c0d90-5c6d-437d-df02-efd2780d7293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.7598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. ROUGE Score\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is used for evaluating automatic summarization and machine translation. It compares the overlap of n-grams, word sequences, and word pairs between the generated text and reference text."
      ],
      "metadata": {
        "id": "3EqO6PgFAwYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW3Oj9IrAWtv",
        "outputId": "70a40024-2d55-435a-b426-33b258cba40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=083e5478a34f74e69e8c2248c44ea5b2a538a17e2f3aa8e8ee224eeb98bda2ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize the ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Example reference and candidate texts\n",
        "reference = \"The quick brown fox jumps over the lazy dog\"\n",
        "candidate = \"The fast brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Compute ROUGE scores\n",
        "scores = scorer.score(reference, candidate)\n",
        "print(f\"ROUGE Scores: {scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw5a_-cyAWr7",
        "outputId": "e478f1b4-6bdc-4d8f-9be6-4855707e59c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores: {'rouge1': Score(precision=0.8888888888888888, recall=0.8888888888888888, fmeasure=0.8888888888888888), 'rouge2': Score(precision=0.75, recall=0.75, fmeasure=0.75), 'rougeL': Score(precision=0.8888888888888888, recall=0.8888888888888888, fmeasure=0.8888888888888888)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "High ROUGE-1 and ROUGE-L scores suggest that the model generates text with good overlap in terms of both individual words and overall structure.\n",
        "\n",
        "ROUGE-2 scores are slightly lower, indicating some challenges in capturing bigram sequences, which is normal and reflects the complexity of generating coherent phrases."
      ],
      "metadata": {
        "id": "lT8XoVfRBvxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Perplexity\n",
        "Perplexity measures how well a probability model predicts a sample. In the context of language models, it helps gauge how well the model understands the text."
      ],
      "metadata": {
        "id": "TCtuKXESA_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhWoWqYLAWp3",
        "outputId": "fc412eaa-9435-4cdd-ee32-2c46810252c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def compute_perplexity(text, tokenizer, model):\n",
        "    # Tokenize and move input tensors to the same device as the model\n",
        "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "    # Ensure model is on the same device as the inputs\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "\n",
        "    loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "\n",
        "    return perplexity.item()\n",
        "\n",
        "# Example text\n",
        "text = \"Once upon a time in a land far, far away\"\n",
        "perplexity = compute_perplexity(text, tokenizer, model)\n",
        "print(f\"Perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQMitAFWBEDI",
        "outputId": "6a0c9a48-2afe-48c5-9bbb-6ae6c20bf133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 21.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Summary\n",
        "## BLEU Score:\n",
        "\n",
        "Score: 0.7598\n",
        "\n",
        "Interpretation: Indicates good overlap between the generated text and reference texts. A higher score suggests that the generated text is quite similar to the reference in terms of word sequences, reflecting effective text generation.\n",
        "\n",
        "## ROUGE Scores:\n",
        "\n",
        "ROUGE-1:\n",
        "Precision: 0.89,\n",
        "Recall: 0.89,\n",
        "F1 Score: 0.89\n",
        "\n",
        "ROUGE-2:\n",
        "Precision: 0.75,\n",
        "Recall: 0.75,\n",
        "F1 Score: 0.75\n",
        "\n",
        "ROUGE-L:\n",
        "Precision: 0.89,\n",
        "Recall: 0.89,\n",
        "F1 Score: 0.89\n",
        "\n",
        "Interpretation: High ROUGE-1 and ROUGE-L scores demonstrate strong performance in capturing word overlap and text structure. The ROUGE-2 scores, while slightly lower, still show good coverage of bigram sequences. Overall, these scores indicate that the generated text closely resembles the reference in both content and structure.\n",
        "\n",
        "## Perplexity:\n",
        "\n",
        "Score: 21.93\n",
        "\n",
        "Interpretation: Reflects the model's ability to predict the next word in the sequence. Lower perplexity generally indicates better performance.\n",
        "\n",
        "A perplexity of 21.93 suggests that the model performs reasonably well but may benefit from further fine-tuning or adjustment to achieve lower values.\n",
        "\n",
        "\n",
        "## Overall Assessment\n",
        "* The BLEU and ROUGE scores indicate high-quality text generation with good similarity to reference texts.\n",
        "* The Perplexity score shows that while the model is performing well, there is room for improvement, particularly in terms of better prediction capabilities.\n",
        "* This evaluation summary should provide a clear view of how well your text generation model is performing.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-qIeUQkGB-aK"
      }
    }
  ]
}